{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b119db8-5ad3-4359-876c-bce1f021fd11",
   "metadata": {},
   "source": [
    "# ALL IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2757ad1-3e05-4cf4-84f7-1736ca785101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import chi2_contingency\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27f78d7-9b17-48f0-be3e-e3098ab6732f",
   "metadata": {},
   "source": [
    "## Loading in dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd47f0fd-ea15-456d-a9fa-b8f25685ffc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_mat = pd.read_table('/home/ics-home/Pearson Proj/Pearson/datasets/student-mat.csv', sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b5a762-77c9-4961-98ad-fb024925f47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a quick look\n",
    "print(student_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bff5d3-73d6-4b17-bf0b-dbe0d47caf80",
   "metadata": {},
   "source": [
    "## 1. Initial look at the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a7a193-ac51-4c97-b58e-a2d0a458c6e8",
   "metadata": {},
   "source": [
    "**Histograms for Quantitative Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5939f277-862c-4dca-9659-139e49fffcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can make individual ones or one for each numeric column\n",
    "# x-axis is the value and y-axis is the count\n",
    "student_mat.hist(bins=20, figsize=(15, 10), color='green', alpha=0.5)\n",
    "plt.suptitle('Histograms for All Numeric Columns in Math df')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a554f31-4ed3-41b9-82ac-afb7e9ed7077",
   "metadata": {},
   "source": [
    "## 2. Visualising Categorical and Quantitaive Vars. vs G3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4596762-ae2f-4802-bc40-c956025d8b4f",
   "metadata": {},
   "source": [
    "**i. Boxplots for Categorical Variables vs G3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986f9258-7fd5-48cd-9e9f-b596b579313c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List of Categorical Variables in Math df\n",
    "cat_vars = [\n",
    "    'sex', 'school', 'address', \n",
    "    'Pstatus', 'Mjob', 'Fjob', \n",
    "    'guardian', 'famsize', 'reason', \n",
    "    'schoolsup', 'famsup', 'activities', \n",
    "    'paid', 'internet', 'nursery', \n",
    "    'higher', 'romantic'\n",
    "]\n",
    "\n",
    "for cat_var in cat_vars:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.boxplot(data=student_mat, x=cat_var, y='G3')\n",
    "    plt.title(f\"G3 by {cat_var}\")\n",
    "    plt.xlabel(f\"{cat_var}\")\n",
    "    plt.ylabel('G3')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b567b9-7567-43dd-abbc-95ed436b4ebc",
   "metadata": {},
   "source": [
    "**ii. Bar Graphs for Quantitative Variables vs G3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505705be-6841-4655-8f39-8446a83f8ad5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List of Numerical Vars in Math df\n",
    "quan_vars = [\n",
    "    \"age\", \"Medu\", \"Fedu\", \"famrel\", \"traveltime\", \"studytime\", \"failures\", \n",
    "    \"freetime\", \"goout\", \"Walc\", \"Dalc\", \"health\", \"absences\", \"G1\", \"G2\"\n",
    "]\n",
    "\n",
    "for quan_var in quan_vars:\n",
    "    g3_means = student_mat.groupby(quan_var)['G3'].mean()\n",
    "    g3_means.plot(kind='bar', title=f\"Average G3 Score by {quan_var}\", figsize=(8,5))\n",
    "    plt.xlabel(f\"{quan_var}\")\n",
    "    plt.ylabel('Average G3 Score')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7162464a-b18b-4c75-882c-c81c407bffda",
   "metadata": {},
   "source": [
    "**ii. Boxplots for Quantitaive Variables vs. G3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bf305d-4325-431e-b335-34f19498483c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for quan_var in quan_vars:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.boxplot(data=student_mat, x=quan_var, y='G3')\n",
    "    plt.title(f\"G3 by {quan_var}\")\n",
    "    plt.xlabel(f\"{quan_var}\")\n",
    "    plt.ylabel('G3')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9e74bb-a44d-43cb-9fba-58fd78b47819",
   "metadata": {},
   "source": [
    "## 3. Picking vars to focus on"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5e34e50-4540-4cb0-924c-2897b56ba828",
   "metadata": {},
   "source": [
    "Based on the boxplots (for the categorical variables) and the scatterplots (for the quantitative variables), we decided to focus on the following variables.\n",
    "\n",
    "# CATEGORICAL VARIABLES\n",
    "- school\n",
    "- address\n",
    "- internet\n",
    "- Mjob\n",
    "- Fjob\n",
    "- schoolsup\n",
    "- higher\n",
    "\n",
    "# QUANTITATIVE VARIABLES\n",
    "- age\n",
    "- Medu\n",
    "- Fedu\n",
    "- traveltime\n",
    "- studytime\n",
    "- failures\n",
    "- health\n",
    "- G1\n",
    "- G2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b65e92-b456-445c-bc9f-881d2dd6ec8a",
   "metadata": {},
   "source": [
    "## 4a. Chi-Square Tests on Categorical Vars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4806d9-8cc4-431b-83ea-e783a60e3329",
   "metadata": {},
   "source": [
    "***For categorical variables, needed to change the G3 scores from integers to letter grades for chi-square test.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c764746f-9fe5-4a4d-ba42-3c346ef9aaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to assign letter grades\n",
    "def assign_grade(score):\n",
    "    if 16 <= score <= 20:\n",
    "        return 'A'\n",
    "    elif 14 <= score <= 15:\n",
    "        return 'B'\n",
    "    elif 12 <= score <= 13:\n",
    "        return 'C'\n",
    "    elif 10 <= score <= 11:\n",
    "        return 'D'\n",
    "    else:\n",
    "        return 'F'\n",
    "\n",
    "# Apply function to each grading period\n",
    "for col, letter_col in zip([\"G1\", \"G2\", \"G3\"], [\"L1\", \"L2\", \"L3\"]):\n",
    "    student_mat[letter_col] = student_mat[col].apply(assign_grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4395a027-88c3-4818-9c6f-29765ea1a1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for running chi-square test\n",
    "def perform_chi_square_test(data, col1, col2):\n",
    "    # creating contingency table\n",
    "    contingency_table = pd.crosstab(data[col1], data[col2])\n",
    "\n",
    "    # performing the chi-square test\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "    # interpreting the result\n",
    "    significant = p < 0.05 #5% significance level\n",
    "    return chi2, p, significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a07f88-d85e-49a6-80d1-f070369b347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Vars of Interest\n",
    "cat_vars_to_test = {\n",
    "    \"Student's School and Academic Performance\": ('school', 'L2'),\n",
    "    \"Student's Address and Academic Performance\": ('address', 'L2'),\n",
    "    \"Internet Access and Academic Performance\": ('internet', 'L2'),\n",
    "    \"Mother's Job and Academic Performance\": ('Mjob', 'L1'),\n",
    "    \"Father's Job and Academic Performance\": ('Fjob', 'L1'),\n",
    "    \"School Support and Academic Performance\": ('schoolsup', 'L3'),\n",
    "    \"Desire for Higher Education and Academic Performance\": ('higher', 'L3')\n",
    "}\n",
    "\n",
    "results = {aspect: perform_chi_square_test(student_mat, *columns) for aspect, columns in cat_vars_to_test.items()}\n",
    "chi_square_results = pd.DataFrame(results)\n",
    "chi_square_results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9fec47af-2410-470b-8b7e-2f4ff7d0f922",
   "metadata": {},
   "source": [
    "From the Chi-Square tests, obtained that 'Mjob', 'Fjob', 'schoolsup', and 'higher' are likely to be correlated with grades. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c161690-0e47-4990-b7a5-ac74781312c7",
   "metadata": {},
   "source": [
    "## 4b. T-Tests on Quantitative Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65193cd5-3a9d-4ebe-88f5-ad77b7e9c616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitaive Vars of Interest\n",
    "quan_vars_focus = [\n",
    "    'age', 'Medu', 'Fedu', 'traveltime', 'studytime', 'failures', 'health', 'G1', 'G2'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32b3201-70a1-4a31-88dc-86f5aa817856",
   "metadata": {},
   "outputs": [],
   "source": [
    "G3_median = student_mat['G3'].median()\n",
    "student_mat['G3_median'] = student_mat['G3'].apply(lambda x: 'low' if x < G3_median else 'high')\n",
    "\n",
    "results = []\n",
    "for quan_var in quan_vars_focus:\n",
    "    low = student_mat[student_mat['G3_median'] == 'low'][quan_var]\n",
    "    high = student_mat[student_mat['G3_median'] == 'high'][quan_var]\n",
    "\n",
    "    t_stat, p_value = stats.ttest_ind(low, high, equal_var = False)\n",
    "\n",
    "    results.append({\"Variable\": quan_var, \"T-test\": t_stat, \"P-value\": p_value})\n",
    "\n",
    "ttest_results = pd.DataFrame(results)\n",
    "ttest_results.sort_values(\"P-value\")\n",
    "\n",
    "ttest_results['Statistically Significant'] = ttest_results['P-value'] < 0.05\n",
    "ttest_results.sort_values('Statistically Significant')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5c3520c7-66a4-48bd-af16-f606d1ac9f49",
   "metadata": {},
   "source": [
    "From the T-Tests, obtained that all but 'health' are likely to be correlated with grades."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0befb35c-ab00-4dcd-b303-785e90736050",
   "metadata": {},
   "source": [
    "# Linear Regression w/ \"Naive Model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eedb44-4b60-4389-8992-cf02b197d7af",
   "metadata": {},
   "source": [
    "**1. Split the data randomly into training (75%) and testing (25%) sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f0bb29-4e68-40b4-a413-cc9a6a842c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of variables we want to focus on (based on visuals and tests)\n",
    "cat_vars = ['Mjob', 'Fjob', 'schoolsup', 'higher']\n",
    "quan_vars = ['age', 'Medu', 'Fedu', 'traveltime', 'studytime', 'failures', 'G1', 'G2']\n",
    "\n",
    "# Make a subset of the variables we want to focus on\n",
    "cat_vars_df = student_mat[cat_vars]\n",
    "quan_vars_df = student_mat[quan_vars]\n",
    "\n",
    "grade = student_mat['G3']\n",
    "\n",
    "# set seed using random_state = 4 to make reproduceable\n",
    "num_train, num_test, grade_train, grade_test = train_test_split(quan_vars_df, grade, random_state = 4)\n",
    "cat_train, cat_test, grade_train, grade_test = train_test_split(cat_vars_df, grade, random_state = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9da465-4761-4fc2-a686-9305fdace632",
   "metadata": {},
   "source": [
    "**2. With training data, fit linear regression models**\n",
    "\n",
    "**3. With testing data, compute RMSE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9335d2-e6c2-4ffa-a30a-4209b56d4fd7",
   "metadata": {},
   "source": [
    "### MIGHT WANT TO USE BARPLOTS SINCE MOST THE VARIABLES (CATEGORICAL AND QUANTITAIVE) ARE DISCRETE\n",
    "\n",
    "### ONLY G1 AND G2 ARE CONTINUOUS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2c559d-124c-4937-a8b3-712f67eb1449",
   "metadata": {},
   "source": [
    "***Categorical Models***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065d908d-10f3-4156-85fe-dec2a4eafdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Create subplots for each categorical variable\n",
    "fig, axes = plt.subplots(nrows=len(cat_vars), ncols=1, figsize=(8, 5 * len(cat_vars)))\n",
    "\n",
    "if len(cat_vars) == 1:\n",
    "    axes = [axes]  # Ensure axes is iterable for a single variable\n",
    "\n",
    "for i, cat_var in enumerate(cat_vars):\n",
    "    # One-hot encode the categorical variable\n",
    "    encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "    cat_train_encoded = encoder.fit_transform(cat_train[[cat_var]])\n",
    "    cat_test_encoded = encoder.transform(cat_test[[cat_var]])\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    cat_train_df = pd.DataFrame(cat_train_encoded, columns=encoder.get_feature_names_out([cat_var]))\n",
    "    cat_test_df = pd.DataFrame(cat_test_encoded, columns=encoder.get_feature_names_out([cat_var]))\n",
    "\n",
    "    # Train Linear Regression model\n",
    "    cat_model = LinearRegression()\n",
    "    cat_model.fit(cat_train_df, grade_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = cat_model.predict(cat_test_df)\n",
    "\n",
    "    # Calculate RMSE instead of MSE\n",
    "    rmse = np.sqrt(mean_squared_error(grade_test, y_pred))\n",
    "    results[cat_var] = rmse\n",
    "    print(f\"Model with {cat_var}: RMSE = {rmse:.4f}\")\n",
    "\n",
    "    # Convert categorical variable to numeric encoding for plotting\n",
    "    label_encoder = LabelEncoder()\n",
    "    cat_test_numeric = label_encoder.fit_transform(cat_test[cat_var])\n",
    "\n",
    "    # Sort values for proper trendline\n",
    "    sorted_indices = np.argsort(cat_test_numeric)\n",
    "    sorted_x = cat_test_numeric[sorted_indices]\n",
    "    sorted_y = y_pred[sorted_indices]\n",
    "\n",
    "    # Plot regression line\n",
    "    axes[i].plot(sorted_x, sorted_y, color=\"red\", linewidth=2)\n",
    "    axes[i].set_title(f'Linear Regression for {cat_var} (RMSE={rmse:.4f})')\n",
    "    axes[i].set_xlabel(cat_var)\n",
    "    axes[i].set_ylabel('Predicted Grades')\n",
    "    axes[i].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5968aa1d-091e-4b6b-b32d-9410f90b629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat_var in cat_vars:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "\n",
    "    # Ensure df_results includes the categorical variable\n",
    "    df_results = pd.DataFrame({\n",
    "        \"Actual\": grade_test.values,\n",
    "        \"Predicted\": y_pred,\n",
    "        cat_var: cat_test[cat_var].values  # Add categorical variable\n",
    "    })\n",
    "\n",
    "    # Group by categorical variable and compute mean\n",
    "    grouped_results = df_results.groupby(cat_var)[[\"Actual\", \"Predicted\"]].mean()\n",
    "\n",
    "    # Plot bar chart\n",
    "    grouped_results.plot(kind=\"bar\", ax=plt.gca(), colormap=\"coolwarm\")\n",
    "\n",
    "    plt.title(f'Average Actual vs. Predicted Grades for {cat_var}')\n",
    "    plt.ylabel('Grade')\n",
    "    plt.xlabel(cat_var)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend([\"Actual\", \"Predicted\"])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b63c38-cd23-4d75-b078-c18e7bbd563b",
   "metadata": {},
   "source": [
    "***Quantitative Models***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251f3c72-1ec8-4688-bb02-d09003fd950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots for each quantitative variable\n",
    "fig, axes = plt.subplots(nrows=len(quan_vars), ncols=1, figsize=(8, 5 * len(quan_vars)))\n",
    "\n",
    "if len(quan_vars) == 1:\n",
    "    axes = [axes]  # Ensure axes is iterable for a single variable\n",
    "\n",
    "for i, quan_var in enumerate(quan_vars):\n",
    "    # Extract the predictor (X) and response (y)\n",
    "    X_train = num_train[[quan_var]].values  # Reshape for sklearn\n",
    "    X_test = num_test[[quan_var]].values\n",
    "    y_train = grade_train.values\n",
    "    y_test = grade_test.values\n",
    "\n",
    "    # Train Linear Regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(f\"Model with {quan_var}: RMSE = {rmse:.4f}\")\n",
    "\n",
    "    # Scatter plot + regression line\n",
    "    sns.regplot(x=X_test.flatten(), y=y_test, scatter=True, ax=axes[i], \n",
    "                line_kws={\"color\": \"red\", \"linewidth\": 2})  # Red regression line\n",
    "\n",
    "    # Customize plot\n",
    "    axes[i].set_title(f'Linear Regression for {quan_var} (RMSE={rmse:.4f})')\n",
    "    axes[i].set_xlabel(quan_var)\n",
    "    axes[i].set_ylabel(\"Actual Grades (G3)\")\n",
    "    axes[i].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5ab746-3d3d-4049-b919-c86338975f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude 'G1' and 'G2' from quantitative variables list\n",
    "quan_vars_filtered = [var for var in quan_vars if var not in ['G1', 'G2']]\n",
    "\n",
    "# Create bar plots for each remaining variable\n",
    "for quan_var in quan_vars_filtered:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "\n",
    "    # Ensure df_results includes the categorical variable\n",
    "    df_results = pd.DataFrame({\n",
    "        \"Actual\": grade_test.values,\n",
    "        \"Predicted\": y_pred,\n",
    "        quan_var: num_test[quan_var].values  # Add variable to group by\n",
    "    })\n",
    "\n",
    "    # Calculate the mean of actual vs predicted grades for each level of the variable\n",
    "    grouped_results = df_results.groupby(quan_var)[[\"Actual\", \"Predicted\"]].mean()\n",
    "\n",
    "    # Plot bar chart\n",
    "    grouped_results.plot(kind=\"bar\", ax=plt.gca(), colormap=\"coolwarm\")\n",
    "\n",
    "    plt.title(f'Average Actual vs. Predicted Grades for {quan_var}')\n",
    "    plt.ylabel('Grade')\n",
    "    plt.xlabel(quan_var)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend([\"Actual\", \"Predicted\"])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09831eed-d65e-4a53-9f4e-ca452908f9d1",
   "metadata": {},
   "source": [
    "## Side Quest: Train a linear regression model using all features (both quantitative and categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0b3c2c-fd10-46c8-a577-35c6de3d6228",
   "metadata": {},
   "source": [
    "***Getting RMSE with all features***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e26d1c8-3322-4786-8e85-84b9c95d1d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine quantitative and categorical data into one DataFrame\n",
    "combined_df = pd.concat([quan_vars_df, cat_vars_df], axis=1)\n",
    "\n",
    "# One-Hot Encode the categorical variables\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "cat_vars_encoded = encoder.fit_transform(cat_vars_df)\n",
    "\n",
    "# Convert the encoded features back to DataFrame\n",
    "cat_vars_encoded_df = pd.DataFrame(cat_vars_encoded, columns=encoder.get_feature_names_out(cat_vars_df.columns))\n",
    "\n",
    "# Combine the one-hot encoded categorical features with the quantitative features\n",
    "combined_df_encoded = pd.concat([quan_vars_df, cat_vars_encoded_df], axis=1)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, grade_train, grade_test = train_test_split(combined_df_encoded, grade, random_state=4)\n",
    "\n",
    "# Train the model using all features (both quantitative and categorical)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, grade_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse = np.sqrt(mean_squared_error(grade_test, y_pred))\n",
    "print(f\"RMSE for the model with all features: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a011dcfb-7254-4b4a-b2a0-c2f0ada10f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r2_score(grade_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f76a648-833a-47a4-9086-86ac93ff4144",
   "metadata": {},
   "source": [
    "***Plotting with all features***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b3096e-40ec-4899-ab6d-23f3becc6c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of actual vs predicted grades\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(grade_test, y_pred, color='blue', alpha=0.5, label=\"Predictions vs Actual\")\n",
    "\n",
    "# Plotting the ideal line where predictions equal actual values (y=x line)\n",
    "plt.plot([grade_test.min(), grade_test.max()], [grade_test.min(), grade_test.max()], color='red', linestyle='--', label=\"Ideal (y=x)\")\n",
    "\n",
    "# Customize plot\n",
    "plt.xlabel('Actual Grades (G3)')\n",
    "plt.ylabel('Predicted Grades')\n",
    "plt.title('Predicted vs Actual Grades (Model with All Features)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9671acd-bd29-492a-bf07-cea203d83948",
   "metadata": {},
   "source": [
    "# Linear Regression w/ \"Smarter Model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727cff77-0567-4842-a521-73e06e2f5330",
   "metadata": {},
   "source": [
    "**1. Forwards Selection Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e9e717-957a-4fcc-95e4-933af5920a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract numerical features\n",
    "quan_vars_df = student_mat[quan_vars]\n",
    "\n",
    "# Extract categorical features\n",
    "cat_vars_df = student_mat[cat_vars]\n",
    "\n",
    "# One-hot encode categorical variables using OneHotEncoder\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False)  \n",
    "cat_encoded = encoder.fit_transform(cat_vars_df)  \n",
    "cat_encoded_df = pd.DataFrame(cat_encoded, columns=encoder.get_feature_names_out(cat_vars_df.columns))\n",
    "\n",
    "# Concatenate numerical and encoded categorical variables\n",
    "X = pd.concat([quan_vars_df, cat_encoded_df], axis=1)\n",
    "\n",
    "# Define target variable\n",
    "y = student_mat['G3']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=4)\n",
    "\n",
    "def forward_selection_with_plot(X_train, y_train, X_test, y_test):\n",
    "    # Initializing variables\n",
    "    remaining_vars = list(X_train.columns)  # All available predictors\n",
    "    selected_vars = []  # Empty list to store selected vars\n",
    "    best_model = None  # Placeholder for the best model\n",
    "    lowest_rmse = float(\"inf\")  # Start with the highest possible RMSE\n",
    "    rmse_list = []  # To store RMSE values at each step\n",
    "    step_list = []  # Stores step count\n",
    "\n",
    "    while remaining_vars:  # Loops until no more variables\n",
    "        rmse_dict = {}\n",
    "        for var in remaining_vars:  # Iterate through each var not yet selected\n",
    "            model_vars = selected_vars + [var]  # Try adding one new var\n",
    "            X_train_selected = sm.add_constant(X_train[model_vars])  # Test the var added\n",
    "            X_test_selected = sm.add_constant(X_test[model_vars])\n",
    "\n",
    "            model = sm.OLS(y_train, X_train_selected).fit()\n",
    "            y_pred = model.predict(X_test_selected)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            rmse_dict[var] = rmse  # Store RMSE for this variable\n",
    "        \n",
    "        best_var = min(rmse_dict, key=rmse_dict.get)  # Choose best var to add\n",
    "        best_rmse = rmse_dict[best_var]\n",
    "\n",
    "        # Check if model improved\n",
    "        if best_rmse < lowest_rmse:\n",
    "            selected_vars.append(best_var)  # Keep this variable\n",
    "            remaining_vars.remove(best_var)  # Remove it from remaining variables\n",
    "            lowest_rmse = best_rmse  # Update best RMSE\n",
    "            best_model = sm.OLS(y_train, sm.add_constant(X_train[selected_vars])).fit()  # Update model\n",
    "            rmse_list.append(best_rmse)\n",
    "            step_list.append(len(selected_vars))\n",
    "            print(f\"Added {best_var}, New RMSE: {best_rmse:.4f}\")\n",
    "        else:\n",
    "            break  # Stop if no more improvements, prevents overfitting\n",
    "\n",
    "    # Plot RMSE vs. Number of Variables Selected\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(step_list, rmse_list, marker='o', linestyle='-')\n",
    "    plt.xlabel(\"Number of Selected Variables\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.title(\"Forward Selection: RMSE vs. Number of Variables\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return best_model, selected_vars\n",
    "\n",
    "# Run forward selection\n",
    "best_forward_model, selected_forward_vars = forward_selection_with_plot(X_train, y_train, X_test, y_test)\n",
    "print(best_forward_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d94d1be-bcef-44e2-812a-0a55e5a58c3f",
   "metadata": {},
   "source": [
    "**2. Backwards Selection Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e24ac4-d14c-47b5-a360-c5480e7a62a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_selection_with_plot(X_train, y_train, X_test, y_test, p_threshold=0.05):\n",
    "    selected_vars = list(X_train.columns)\n",
    "    rmse_list = []  # Track RMSE\n",
    "    step_list = []  # Track step number\n",
    "\n",
    "    while len(selected_vars) > 0:\n",
    "        X_train_selected = sm.add_constant(X_train[selected_vars])\n",
    "        X_test_selected = sm.add_constant(X_test[selected_vars])\n",
    "        \n",
    "        model = sm.OLS(y_train, X_train_selected).fit()\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        \n",
    "        rmse_list.append(rmse)\n",
    "        step_list.append(len(selected_vars))\n",
    "\n",
    "        p_values = model.pvalues[1:]  # Exclude intercept\n",
    "        worst_var = p_values.idxmax()\n",
    "        worst_pval = p_values.max()\n",
    "\n",
    "        if worst_pval > p_threshold:\n",
    "            print(f\"Removing {worst_var} (p-value={worst_pval:.4f})\")\n",
    "            selected_vars.remove(worst_var)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Plot RMSE vs. Number of Remaining Variables\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(step_list, rmse_list, marker='o', linestyle='-')\n",
    "    plt.xlabel(\"Number of Remaining Variables\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.title(\"Backward Selection: RMSE vs. Number of Variables\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    final_model = sm.OLS(y_train, sm.add_constant(X_train[selected_vars])).fit()\n",
    "    return final_model, selected_vars\n",
    "\n",
    "best_backward_model, selected_backward_vars = backward_selection_with_plot(X_train, y_train, X_test, y_test)\n",
    "print(best_backward_model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d211955-7783-47b4-a24e-e50dcf906b51",
   "metadata": {},
   "source": [
    "***Plotting...***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6666243e-4cff-42aa-9ca1-fd583440ef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(model, X_test, y_test, selected_vars, title):\n",
    "    X_test_selected = sm.add_constant(X_test[selected_vars])\n",
    "    y_pred = model.predict(X_test_selected)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.6, label=\"Predictions\")\n",
    "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color=\"red\", linestyle=\"--\", label=\"Perfect Fit\")\n",
    "    \n",
    "    plt.xlabel(\"Actual Grades (G3)\")\n",
    "    plt.ylabel(\"Predicted Grades (G3)\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Forward selection model predictions\n",
    "plot_predictions(best_forward_model, X_test, y_test, selected_forward_vars, \"Forward Selection: Predicted vs Actual\")\n",
    "\n",
    "# Backward selection model predictions\n",
    "plot_predictions(best_backward_model, X_test, y_test, selected_backward_vars, \"Backward Selection: Predicted vs Actual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5caebc2-cf83-4f3a-8ddf-5364df578e61",
   "metadata": {},
   "source": [
    "**3. Stepwise Selection Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3432eb-b405-498a-81e1-1d4b40a2051d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepwise_selection(X_train, y_train, X_test, y_test, direction='both'):\n",
    "    \"\"\"\n",
    "    Perform stepwise selection based on RMSE.\n",
    "    direction: 'forward', 'backward', or 'both'\n",
    "    \"\"\"\n",
    "    initial_features = X_train.columns.tolist()\n",
    "    selected_features = []\n",
    "    best_model = None\n",
    "    lowest_rmse = float('inf')  # Start with a very high RMSE\n",
    "\n",
    "    while True:\n",
    "        changed = False\n",
    "        \n",
    "        # Forward Selection\n",
    "        if direction in ['forward', 'both']:\n",
    "            remaining_features = [f for f in initial_features if f not in selected_features]\n",
    "            for feature in remaining_features:\n",
    "                model = sm.OLS(y_train, sm.add_constant(X_train[selected_features + [feature]])).fit()\n",
    "                y_pred = model.predict(sm.add_constant(X_test[selected_features + [feature]]))\n",
    "                rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "                if rmse < lowest_rmse:\n",
    "                    lowest_rmse = rmse\n",
    "                    best_model = model\n",
    "                    selected_features.append(feature)\n",
    "                    changed = True\n",
    "                    print(f\"Added {feature}, New RMSE: {rmse:.4f}\")\n",
    "\n",
    "        # Backward Elimination\n",
    "        if direction in ['backward', 'both']:\n",
    "            for feature in selected_features:\n",
    "                temp_features = selected_features.copy()\n",
    "                temp_features.remove(feature)\n",
    "                model = sm.OLS(y_train, sm.add_constant(X_train[temp_features])).fit()\n",
    "                y_pred = model.predict(sm.add_constant(X_test[temp_features]))\n",
    "                rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "                if rmse < lowest_rmse:\n",
    "                    lowest_rmse = rmse\n",
    "                    best_model = model\n",
    "                    selected_features.remove(feature)\n",
    "                    changed = True\n",
    "                    print(f\"Removed {feature}, New RMSE: {rmse:.4f}\")\n",
    "\n",
    "        # If no changes were made, stop the procedure\n",
    "        if not changed:\n",
    "            break\n",
    "    \n",
    "    # Print final RMSE on the test set\n",
    "    print(f\"Final RMSE on test set: {lowest_rmse:.4f}\")\n",
    "\n",
    "    # Plot RMSE over time (number of features selected)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(1, len(selected_features) + 1), [lowest_rmse] * len(selected_features), marker='o', linestyle='-')\n",
    "    plt.xlabel(\"Number of Features Selected\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.title(\"Stepwise Selection: RMSE vs. Number of Features\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return best_model, selected_features\n",
    "\n",
    "# Run the stepwise selection using RMSE as the criterion\n",
    "best_stepwise_model, selected_stepwise_vars = stepwise_selection(X_train, y_train, X_test, y_test, direction='both')\n",
    "print(best_stepwise_model.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
